<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>promql on 阳明的博客</title>
    <link>https://www.qikqiak.com/tags/promql/</link>
    <description>Recent content in promql on 阳明的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.qikqiak.com/tags/promql/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prometheus 监控 Kubernetes Job 资源误报的坑</title>
      <link>https://www.qikqiak.com/post/prometheus-monitor-k8s-job-trap/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.qikqiak.com/post/prometheus-monitor-k8s-job-trap/</guid>
      <description>&lt;p&gt;昨天在 Prometheus 课程辅导群里面有同学提到一个问题，是关于 Prometheus 监控 Job 任务误报的问题，大概的意思就 CronJob 控制的 Job，前面执行失败了，监控会触发报警，解决后后面生成的新的 Job 可以正常执行了，但是还是会收到前面的报警：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picdn.youdianzhishi.com/images/20220305200158.png&#34; alt=&#34;问题描述&#34;&gt;&lt;/p&gt;
&lt;p&gt;这是因为一般在执行 Job 任务的时候我们会保留一些历史记录方便排查问题，所以如果之前有失败的 Job 了，即便稍后会变成成功的，那么之前的 Job 也会继续存在，而大部分直接使用 kube-prometheus 安装部署的话使用的默认报警规则是&lt;code&gt;kube_job_status_failed &amp;gt; 0&lt;/code&gt;，这显然是不准确的，只有我们去手动删除之前这个失败的 Job 任务才可以消除误报，当然这种方式是可以解决问题的，但是不够自动化，一开始没有想得很深入，想去自动化删除失败的 Job 来解决，但是这也会给运维人员带来问题，就是不方便回头去排查问题。下面我们来重新整理下思路解决下这个问题。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
